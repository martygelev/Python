{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be easier, will divide the algorithm into several parts:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ol type=\"1\">\n",
    "  <li>Introduction</li>\n",
    "  <li>Handle Data</li>\n",
    "  <li>Summarize Data</li>\n",
    "  <li>Make a Prediction</li>\n",
    "  <li>Make Predictions</li>\n",
    "  <li>Evaluate Accuracy</li>\n",
    "  <li>Tie it Together</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  What is Naive Bayes algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>A Naive Bayes is an algorithm that uses Bayes' theorem to classify objects. Popular uses include spam filters, text analysis and medical diagnosis and also in machine learning because they are simple to implement. In probability theory and statistics Bayes' theorem or Bayes' rule discribes the probability of an event based on prior knowledge of conditions that might be related to the event. This model is very useful for large dataset. For example let's imagine that we want to play football. But is it a good weather for football? Is it too hot or is it rain? Let's take weather forecast for next two weeks.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formula is:\n",
    "$$ P(A|C)=\\frac{P(C|A)P(A)}{P(C)} $$\n",
    "\n",
    "<ul style=\"list-style-type:disc\">\n",
    "  <li>P(A | C) is a conditional probability: the likelihood of event A occurring given that C is true.</li>\n",
    "  <li>P(A) is the prior probability of class</li>\n",
    "  <li>P(C | A) on the contrary P(A | C)</li>\n",
    "    <li>P(C) is the prior probability of predictor</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset:\n",
    "\n",
    "| outlook  | temperature | humidity | windy | play |\n",
    "|----------|-------------|----------|-------|------|\n",
    "| sunny    | hot         | high     | false | no   |\n",
    "| sunny    | hot         | high     | true  | no   |\n",
    "| overcast | hot         | high     | false | yes  |\n",
    "| rainy    | mild        | high     | false | yes  |\n",
    "| rainy    | cool        | normal   | false | yes  |\n",
    "| rainy    | cool        | normal   | true  | no   |\n",
    "| overcast | cool        | normal   | true  | yes  |\n",
    "| sunny    | mild        | high     | false | no   |\n",
    "| sunny    | cool        | normal   | false | yes  |\n",
    "| rainy    | mild        | normal   | false | yes  |\n",
    "| sunny    | mild        | normal   | true  | yes  |\n",
    "| overcast | mild        | high     | true  | yes  |\n",
    "| overcast | hot         | normal   | false | yes  |\n",
    "| rainy    | mild        | high     | true  | no   |\n",
    "\n",
    "\n",
    "\n",
    "<p>Now also need to calculate individual probabilities which are Outlook, Temperature, Humidity, Wind and total (play).Let's try with sunny and cool temperature with high humidlity and without wind.</p>\n",
    "\n",
    "\n",
    "<h3> Probability that we can play. </h3>\n",
    "<br>\n",
    "\n",
    "P(Outlook=sunny | Play = yes) = 2/9<br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "P(Temperature=cool | Play = yes) = 3/9<br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "P(Humidlity=high   | Play = yes) = 3/9<br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "P(Wind = false     | Play = yes) = 3/9<br>\n",
    "\n",
    "\n",
    "P(Play=Yes) = 9/14<br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<h3> Probability we cannot play.</h3>\n",
    "\n",
    "\n",
    "P(Outlook=sunny | Play = no) = 3/5<br>\n",
    "\n",
    "P(Temperature=cool | Play = no) = 1/5<br>\n",
    "\n",
    "P(Humidlity=high   | Play = no) = 4/5<br>\n",
    "\n",
    "P(Wind = false     | Play = no) = 3/5<br>\n",
    "\n",
    "P(Play=NO) = 5/14<br>\n",
    "\n",
    "\n",
    "Let's calculate this.\n",
    "   \n",
    "      P(a | c)P(c)\n",
    "   \n",
    "      P(a | Play=Yes) * P(Yes) = (2/9)*(3/9)*(3/9)*(3/9)*(9/14)=0.053\n",
    "    \n",
    "    \n",
    "    now to calculate for NO\n",
    "    \n",
    "    \n",
    "    P(a | play = no)*P(No) = (3/5)*(1/5)*(4/5)*(3/5)*(5/14)=0.0206\n",
    "\n",
    "Divide both sides to by evidence P(x) to normalize. The evidence for both equations is the same.\n",
    "\n",
    "\n",
    "P(x) = P(outlook = sunny)*P(teperature = cool)*P(humidlity = high)*P(wind = false)<br>\n",
    "\n",
    "\n",
    "P(x) = (5/14)*(4/14)*(7/14)*(6/14) = 0.02186<br>\n",
    "\n",
    "\n",
    "<h5>Divide the results by this value:</h5>\n",
    "\n",
    " P(play=yes | x) = 0.2424<br>\n",
    " \n",
    " P(play=no | x ) = 0.9421<br>\n",
    " \n",
    "<span><bold>0.9421>0.2424</bold></span> so we cannot play football today.<br>\n",
    "\n",
    "\n",
    "\n",
    "<italic>Now I will try to implement this algorithm from scratch in few steps.</italic>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01. Handle Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The test problem is the <strong>Pima Indians Diabetes  problem</strong> . We can also take the famous <ins><i>Titanic Disaster dataset</i></ins>\n",
    "     (It gathers Titanic passenger personal information and whether or not they survived to the shipwreck.)\n",
    "This problem contains 768 measurements such as their age, the number of times pregnant and blood workup.<p>\n",
    "    \n",
    "<p>Each record shows if the patient suffered from diabet in last 5 years.This is a standard dataset that has been studied a lot in machine learning literature. A good prediction accuracy is 70%-76%.</p>\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to load our data file. Also need to convert strings into numbers that can work with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "import math\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadCsv(filename):\n",
    "    lines=csv.reader(open(filename))\n",
    "    data=list(lines)\n",
    "    for num in range(len(data)):\n",
    "        data[num]=[float(number) for number in data[num]]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test this function by loalding the <strong><ins>Prima indians dataset</ins></strong> and printing the numbers of instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data file pima-indians-diabetes.data.csv with 768 rows\n"
     ]
    }
   ],
   "source": [
    "filename='pima-indians-diabetes.data.csv'\n",
    "dataset=loadCsv(filename)\n",
    "print(('Loaded data file {0} with {1} rows').format(filename,len(dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to split the data into train datasets that Naive Bayes to make predictions. Let's split it with ratio 67% train and 33% TEST(<i>I FOUND THIS IN COMMON RATIO TESTING ALGORITHM ON A DATASET</i>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def splitData(dataset, splitRatio):\n",
    "    rowSize=int(len(dataset)*splitRatio)\n",
    "    rowSet =[]\n",
    "    copyData=list(dataset)\n",
    "    while len(rowSet)<rowSize:\n",
    "        index=random.randrange(len(copyData))\n",
    "        rowSet.append(copyData.pop(index))\n",
    "    return [rowSet,copyData]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 7 rows into train with [[5], [3], [6], [2]] and test with [[1], [4], [7]]\n"
     ]
    }
   ],
   "source": [
    "dataset=[[1],[2],[3],[4],[5],[6],[7]]\n",
    "splitRatio=0.67\n",
    "train,test=splitData(dataset,splitRatio)\n",
    "print(('Split {0} rows into train with {1} and test with {2}').format(len(dataset),train,test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02.Summarize Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>This summary is used when making predictions. The summary of data collected require the mean and the standart deviation for each, by class value.</p>\n",
    "\n",
    "<p>We can break the preparation of this summary into sub-tasks:</p>\n",
    "#### 1 - Separate Data By Class\n",
    "#### 2 - Calculate Mean\n",
    "#### 3 - Calculate Standard Deviation\n",
    "#### 4 - Summarize Dataset\n",
    "#### 5 - Summarize Attributes By Class "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1- Separate Data by class\n",
    "\n",
    "<p> The main point is to calculate statistics for each class. By creating a map of each class value to a list of instances that belong to that class and sort dataset into oredered list.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitByClass(dataset):\n",
    "    splitted = {}\n",
    "    for i in range(len(dataset)):\n",
    "        vector = dataset[i]\n",
    "        if(vector[-1] not in splitted): # check  last item \n",
    "            splitted[vector[-1]]=[] # new class\n",
    "        splitted[vector[-1]].append(vector) # add it to class\n",
    "    return splitted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the function assumes that the last attribute (-1) is the class value. The function returns a map of class values to lists of data instances.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitted instances: {5: [[1, 3, 5]], 6: [[2, 4, 6], [6, 6, 6], [333, 1111, 6]], 11: [[7, 9, 11]], 1: [[7, 2, 1]], 33: [[6, 22, 33]]}\n"
     ]
    }
   ],
   "source": [
    "dataset = [[1,3,5],[2,4,6],[7,9,11] , [7,2,1] , [6,22,33] , [6,6,6] , [333,1111,6]]\n",
    "splitted = splitByClass(dataset)\n",
    "print(('Splitted instances: {0}').format(splitted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 - Calculate Mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def medium(numbers):\n",
    "    return sum(numbers)/float(len(numbers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will be used as the middle of gaussian distribution when calculating probabilitues.\n",
    "We can test this by taking the mean of the numbers 1 to 5.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average of numbers [1, 2, 3, 4, 5] is 3.0\n"
     ]
    }
   ],
   "source": [
    "numbers = [1,2,3,4,5]\n",
    "print(('Average of numbers {0} is {1}').format(numbers, medium(numbers)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 - Calculate Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardDeviation(numbers):\n",
    "    average = mean(numbers)\n",
    "    variance = sum([pow(x-average,2) for x in numbers])/float(len(numbers)-1) \n",
    "    return math.sqrt(variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard deviation is calculated as the square root of the variance. The variance is calculate as average of the squred differences for each attribute value from the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Deviation of [1, 2, 3, 4, 5] is 1.5811388300841898\n"
     ]
    }
   ],
   "source": [
    "numbers=[1,2,3,4,5]\n",
    "standardDeviation(numbers)\n",
    "print(('Standard Deviation of {0} is {1}').format(numbers,standardDeviation(numbers)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4 - Summarize Dataset\n",
    "\n",
    "Now we can summarize dataset and calculate for each attribute.\n",
    "<p> The <strong>ZIP</strong> function groups the values for each attribute across data instances into their own list. That can compute the medium(average) and standard deviation values for the attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarizeData(dataset):\n",
    "    summaries=[(medium(attribute), standardDeviation(attribute)) for attribute in zip(*dataset)]\n",
    "    del summaries[-1] #assign None to it\n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attribute summaries: [(2.0, 1.0), (78.0, 105.77334257741882), (2.3333333333333335, 1.5275252316519465)]\n"
     ]
    }
   ],
   "source": [
    "dataset = [[1,200,1,3], [2,12,2,4], [3,22,4,5]]\n",
    "summary = summarizeData(dataset)\n",
    "print(('Attribute summaries: {0}').format(summary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 - Summarize Attributes By Class\n",
    "All of the previous steps can be pulled together by first separating our training data into instances grouped by class and then calculate the summaries for each attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarizeByClass(dataset):\n",
    "    splitted = splitByClass(dataset)\n",
    "    print(splitted.items())\n",
    "    summaries = {}\n",
    "    for classVal, instances in splitted.items():\n",
    "        summaries[classVal] = summarizeData(instances)\n",
    "    return summaries\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([(1, [[1, 20, 1], [3, 22, 1]]), (0, [[2, 21, 0], [4, 22, 0]])])\n",
      "Summary by class value: {1: [(2.0, 1.4142135623730951), (21.0, 1.4142135623730951)], 0: [(3.0, 1.4142135623730951), (21.5, 0.7071067811865476)]}\n"
     ]
    }
   ],
   "source": [
    "dataset = [[1,20,1], [2,21,0], [3,22,1], [4,22,0]]\n",
    "summary = summarizeByClass(dataset)\n",
    "print(('Summary by class value: {0}').format(summary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03.Make Prediction\n",
    "Dividing this part into few steps will be easier.\n",
    "\n",
    "<p> The result is the conditional probability of a given attribute value given a class value.\n",
    "    In the next class \"calculateProbability\" first calculate the exponent and then the main division </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateProbability(x,medium,standardDeviation):\n",
    "    exponent = math.exp(-(math.pow(x-medium,2)/(2*math.pow(standardDeviation,2)))) # e**x\n",
    "    print(exponent)\n",
    "    return (1 / (math.sqrt(2*math.pi) * standardDeviation)) * exponent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9711577240975457\n",
      "Probability of belonging to this class: 0.06248965759370005\n"
     ]
    }
   ],
   "source": [
    "x = 71.5\n",
    "medium = 73\n",
    "standardDeviation= 6.2\n",
    "probability = calculateProbability(x, medium, standardDeviation)\n",
    "print(('Probability of belonging to this class: {0}').format(probability))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate class probability\n",
    "In next class 'calculateClassProbabilities' combine probabilities together by using multiplication.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcClassProb(summaries,vector):\n",
    "    probabilities={}\n",
    "    #print(summaries.items())\n",
    "\n",
    "    for classValue, summarisedClass in summaries.items():\n",
    "        probabilities[classValue] = 1 \n",
    "        for i in range(len(summarisedClass)):\n",
    "            medium, standardDeviation = summarisedClass[i]\n",
    "            #print(summarisedClass[i])\n",
    "            x=vector[i]\n",
    "            \n",
    "            probabilities[classValue] *= calculateProbability(x, medium, standardDeviation)\n",
    "            print( probabilities[classValue] )\n",
    "    return probabilities\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9801986733067553\n",
      "0.7820853879509118\n",
      "0.0007894295199561683\n",
      "6.298736258150442e-05\n",
      "Probabilities for each class: {0: 0.7820853879509118, 1: 6.298736258150442e-05}\n"
     ]
    }
   ],
   "source": [
    "summaries = {0:[(1, 0.5)], 1:[(20, 5.0)]}\n",
    "vector = [1.1, '?']\n",
    "probabilities = calcClassProb(summaries, vector)\n",
    "print(('Probabilities for each class: {0}').format(probabilities))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make Prediction\n",
    "<p> Now can calculate the probability of a data instance belonging to pach class value it's possible to look for largest probability and return the associated class.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makePrediction(summaries, vector):\n",
    "    probabilities = calcClassProb(summaries, vector)\n",
    "    label , highestProb=None,-1\n",
    "    for classValue, probability in probabilities.items():\n",
    "       # print(('classvalue: {0}').format(classValue))\n",
    "        if label is None or probability > highestProb:\n",
    "                highestProb=probability\n",
    "                print(('highest probability: {0} ').format(highestProb))\n",
    "                label = classValue\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9801986733067553\n",
      "0.7820853879509118\n",
      "0.0007894295199561683\n",
      "6.298736258150442e-05\n",
      "highest probability: 0.7820853879509118 \n",
      "Prediction: A\n"
     ]
    }
   ],
   "source": [
    "summaries = {'A':[(1, 0.5)], 'B':[(20, 5.0)]}\n",
    "inputVector = [1.1, '?']\n",
    "result = makePrediction(summaries, inputVector)\n",
    "print(('Prediction: {0}').format(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04. Make Predictions\n",
    "\n",
    "<p> Finally, for each instance in that dataset, it's possible to estimate the accuraccy of the model by making predictions.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finalPrediction (summaries, testSet):\n",
    "    predictions = []\n",
    "    for i in range(len(testSet)):\n",
    "        result = makePrediction(summaries,testSet[i])\n",
    "        predictions.append(result)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9801986733067553\n",
      "0.7820853879509118\n",
      "0.0007894295199561683\n",
      "6.298736258150442e-05\n",
      "highest probability: 0.7820853879509118 \n",
      "2.7642006664904113e-285\n",
      "2.2055130347536897e-285\n",
      "0.9839305142725084\n",
      "0.0785062966240858\n",
      "highest probability: 2.2055130347536897e-285 \n",
      "highest probability: 0.0785062966240858 \n",
      "Predictions: ['A', 'B']\n"
     ]
    }
   ],
   "source": [
    "summaries = {'A':[(1, 0.5)], 'B':[(20, 5.0)]}\n",
    "testSet = [[1.1, '?'], [19.1, '?']]\n",
    "predictions = finalPrediction(summaries, testSet)\n",
    "print(('Predictions: {0}').format(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 05. Get Accuracy\n",
    "\n",
    "<p> The predictions can be compared to the class values in the test dataset and classification accuracy can be calculated as an accuracy ratio between 0% and 100%</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(testSet, predictions):\n",
    "    correct =0\n",
    "    for i in range (len(testSet)):\n",
    "        if testSet[i][-1] == predictions[i]:\n",
    "            correct+=1\n",
    "    return (correct/float(len(testSet)))*100.0\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 66.66666666666666\n"
     ]
    }
   ],
   "source": [
    "testSet = [[1,1,1,'a'], [2,2,2,'z'], [3,3,3,'q']]\n",
    "predictions = ['a', 'z', 'a']\n",
    "accuracy = accuracy(testSet, predictions)\n",
    "print(('Accuracy: {0}').format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally... tying every single step into one part will be something like :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 74.40944881889764%\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import random\n",
    "import math\n",
    "import gzip\n",
    "import random\n",
    "\n",
    "def loadCsv(filename):\n",
    "    lines=csv.reader(open(filename))\n",
    "    data=list(lines)\n",
    "    for num in range(len(data)):\n",
    "        data[num]=[float(number) for number in data[num]]\n",
    "    return data\n",
    "\n",
    "def splitData(dataset, splitRatio):\n",
    "    rowSize=int(len(dataset)*splitRatio)\n",
    "    rowSet =[]\n",
    "    copyData=list(dataset)\n",
    "    while len(rowSet)<rowSize:\n",
    "        index=random.randrange(len(copyData))\n",
    "        rowSet.append(copyData.pop(index))\n",
    "    return [rowSet,copyData]\n",
    "\n",
    "def splitByClass(dataset):\n",
    "    splitted = {}\n",
    "    for i in range(len(dataset)):\n",
    "        vector = dataset[i]\n",
    "        if(vector[-1] not in splitted): # check  last item \n",
    "            splitted[vector[-1]]=[] # new class\n",
    "        splitted[vector[-1]].append(vector) # add it to class\n",
    "    return splitted\n",
    "\n",
    "def medium(numbers):\n",
    "    return sum(numbers)/float(len(numbers))\n",
    "\n",
    "def standardDeviation(numbers):\n",
    "    average = mean(numbers)\n",
    "    variance = sum([pow(x-average,2) for x in numbers])/float(len(numbers)-1) \n",
    "    return math.sqrt(variance)\n",
    "\n",
    "def summarizeData(dataset):\n",
    "    summaries=[(medium(attribute), standardDeviation(attribute)) for attribute in zip(*dataset)]\n",
    "    del summaries[-1] #assign None to it\n",
    "    return summaries\n",
    "\n",
    "def summarizeByClass(dataset):\n",
    "    splitted = splitByClass(dataset)\n",
    "   # print(splitted.items())\n",
    "    summaries = {}\n",
    "    for classVal, instances in splitted.items():\n",
    "        summaries[classVal] = summarizeData(instances)\n",
    "    return summaries\n",
    "\n",
    "def calculateProbability(x,medium,standardDeviation):\n",
    "    exponent = math.exp(-(math.pow(x-medium,2)/(2*math.pow(standardDeviation,2)))) # e**x\n",
    "    #print(exponent)\n",
    "    return (1 / (math.sqrt(2*math.pi) * standardDeviation)) * exponent\n",
    "\n",
    "def calcClassProb(summaries,vector):\n",
    "    probabilities={}\n",
    "    #print(summaries.items())\n",
    "\n",
    "    for classValue, summarisedClass in summaries.items():\n",
    "        probabilities[classValue] = 1 \n",
    "        for i in range(len(summarisedClass)):\n",
    "            medium, standardDeviation = summarisedClass[i]\n",
    "            #print(summarisedClass[i])\n",
    "            x=vector[i]\n",
    "            \n",
    "            probabilities[classValue] *= calculateProbability(x, medium, standardDeviation)\n",
    "           # print( probabilities[classValue] )\n",
    "    return probabilities\n",
    "    \n",
    "def makePrediction(summaries, vector):\n",
    "    probabilities = calcClassProb(summaries, vector)\n",
    "    label , highestProb=None,-1\n",
    "    for classValue, probability in probabilities.items():\n",
    "       # print(('classvalue: {0}').format(classValue))\n",
    "        if label is None or probability > highestProb:\n",
    "                highestProb=probability\n",
    "               # print(('highest probability: {0} ').format(highestProb))\n",
    "                label = classValue\n",
    "    return label\n",
    "\n",
    "def finalPrediction (summaries, testSet):\n",
    "    predictions = []\n",
    "    for i in range(len(testSet)):\n",
    "        result = makePrediction(summaries,testSet[i])\n",
    "        predictions.append(result)\n",
    "    return predictions\n",
    "\n",
    "def accuracy(testSet, predictions):\n",
    "    correct =0\n",
    "    for i in range (len(testSet)):\n",
    "        if testSet[i][-1] == predictions[i]:\n",
    "            correct+=1\n",
    "    return (correct/float(len(testSet)))*100.0\n",
    "\n",
    "def main():\n",
    "    filename = 'pima-indians-diabetes.data.csv'\n",
    "    splitRatio = 0.67\n",
    "    dataset = loadCsv(filename)\n",
    "    trainingSet, testSet = splitDataset(dataset, splitRatio)\n",
    "   # print(('Split {0} rows into train={1} and test={2} rows').format(len(dataset), len(trainingSet), len(testSet)))\n",
    "    summaries = summarizeByClass(dataset)\n",
    "    predictions = getPredictions(summaries, testSet)\n",
    "    accuracy = getAccuracy(testSet, predictions)\n",
    "    print(('Accuracy: {0}%').format(accuracy))\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "http://dataaspirant.com/2017/02/06/naive-bayes-classifier-machine-learning/\n",
    "\n",
    "\n",
    "https://www.nltk.org/\n",
    "\n",
    "https://en.wikipedia.org/wiki/Bayes%27_theorem\n",
    "\n",
    "\n",
    "https://www.python-course.eu/naive_bayes_classifier_introduction.php\n",
    "\n",
    "http://scikit-learn.org/stable/modules/naive_bayes.html\n",
    "\n",
    "https://docs.python.org/3.3/library/functions.html\n",
    "\n",
    "https://stackoverflow.com/questions/13704860/zip-lists-in-python - about zip function\n",
    "\n",
    "https://docs.python.org/2/library/math.html  - Mathematical functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.ranks.nl/stopwords \n",
    "stop Words\n",
    "https://github.com/huned/node-stopwords stopwords lib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk # use natural language toolkit - braking up sentances into words; reducing their stem\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "# word stemmer\n",
    "stemmer = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 sentences of training data\n"
     ]
    }
   ],
   "source": [
    "# 3 classes of training data\n",
    "training_data = []\n",
    "training_data.append({\"class\":\"greeting\", \"sentence\":\"how are you?\"})\n",
    "training_data.append({\"class\":\"greeting\", \"sentence\":\"how is your day?\"})\n",
    "training_data.append({\"class\":\"greeting\", \"sentence\":\"good day\"})\n",
    "training_data.append({\"class\":\"greeting\", \"sentence\":\"how is it going today?\"})\n",
    "\n",
    "training_data.append({\"class\":\"goodbye\", \"sentence\":\"have a nice day\"})\n",
    "training_data.append({\"class\":\"goodbye\", \"sentence\":\"see you later\"})\n",
    "training_data.append({\"class\":\"goodbye\", \"sentence\":\"have a nice day\"})\n",
    "training_data.append({\"class\":\"goodbye\", \"sentence\":\"talk to you soon\"})\n",
    "\n",
    "training_data.append({\"class\":\"sandwich\", \"sentence\":\"make me a sandwich\"})\n",
    "training_data.append({\"class\":\"sandwich\", \"sentence\":\"can you make a sandwich?\"})\n",
    "training_data.append({\"class\":\"sandwich\", \"sentence\":\"having a sandwich today?\"})\n",
    "training_data.append({\"class\":\"sandwich\", \"sentence\":\"what's for lunch?\"})\n",
    "print (\"%s sentences of training data\" % len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/martingelev/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Corpus words and counts: {'how': 3, 'ar': 1, 'you': 4, 'is': 2, 'yo': 1, 'day': 4, 'good': 1, 'it': 1, 'going': 1, 'today': 2, 'hav': 3, 'a': 5, 'nic': 2, 'see': 1, 'lat': 1, 'talk': 1, 'to': 1, 'soon': 1, 'mak': 2, 'me': 1, 'sandwich': 3, 'can': 1, 'what': 1, 'for': 1, 'lunch': 1} \n",
      "\n",
      "Class words: {'sandwich': ['mak', 'me', 'a', 'sandwich', 'can', 'you', 'mak', 'a', 'sandwich', 'hav', 'a', 'sandwich', 'today', 'what', 'for', 'lunch'], 'goodbye': ['hav', 'a', 'nic', 'day', 'see', 'you', 'lat', 'hav', 'a', 'nic', 'day', 'talk', 'to', 'you', 'soon'], 'greeting': ['how', 'ar', 'you', 'how', 'is', 'yo', 'day', 'good', 'day', 'how', 'is', 'it', 'going', 'today']}\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "# capture unique stemmed words in the training corpus\n",
    "corpus_words = {} # dictionary each stemmed word and the # of occurances\n",
    "class_words = {} # each class and the list of stemmed words within it \n",
    "# turn a list into a set (of unique items) and then a list again \n",
    "classes = list(set([a['class'] for a in training_data]))\n",
    "for c in classes:\n",
    "    # prepare a list of words within each class\n",
    "    class_words[c] = []\n",
    "\n",
    "# loop through each sentence in our training data\n",
    "for data in training_data:\n",
    "    # tokenize each sentence into words\n",
    "    for word in nltk.word_tokenize(data['sentence']):\n",
    "        # ignore a some things\n",
    "        if word not in [\"?\", \"'s\"]:\n",
    "            # stem and lowercase each word\n",
    "            stemmed_word = stemmer.stem(word.lower())\n",
    "            # have we not seen this word already?\n",
    "            if stemmed_word not in corpus_words:\n",
    "                corpus_words[stemmed_word] = 1\n",
    "            else:\n",
    "                corpus_words[stemmed_word] += 1\n",
    "\n",
    "            # add the word to our words in class list\n",
    "            class_words[data['class']].extend([stemmed_word])\n",
    "\n",
    "# we now have each stemmed word and the number of occurances of the word in our training corpus (the word's commonality)\n",
    "print (\"Corpus words and counts: %s \\n\" % corpus_words)\n",
    "# also we have all words in each class\n",
    "print (\"Class words: %s\" % class_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is algorithm. Each word with be equal weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate a score for a given class\n",
    "def calculate_class_score(sentence, class_name, show_details=True):\n",
    "    score = 0 \n",
    "    # tokenize each word in our new sentence\n",
    "    for word in nltk.word_tokenize(sentence):\n",
    "        # check to see if the stem of the word is in any of our classes\n",
    "        if stemmer.stem(word.lower()) in class_words[class_name]:\n",
    "            # treat each word with same weight\n",
    "            score += 1\n",
    "            \n",
    "            if show_details:\n",
    "                print (\"match: %s\" % stemmer.stem(word.lower()))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match: for\n",
      "match: hav\n",
      "match: lunch\n",
      "Class: sandwich  Score: 3 \n",
      "\n",
      "match: day\n",
      "match: to\n",
      "match: hav\n",
      "Class: goodbye  Score: 3 \n",
      "\n",
      "match: good\n",
      "match: day\n",
      "Class: greeting  Score: 2 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we can now calculate a score for a new sentence\n",
    "sentence = \"good day for us to having lunch ?\"\n",
    "\n",
    "# now we can find the class with the highest score\n",
    "for c in class_words.keys():\n",
    "    print (\"Class: %s  Score: %s \\n\" % (c, calculate_class_score(sentence, c)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some words should carry lower weigh if they are more common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate a score for a given class taking into account word commonality\n",
    "def calculate_class_score_commonality(sentence, class_name, show_details=True):\n",
    "    score = 0\n",
    "    # tokenize each word in our new sentence\n",
    "    for word in nltk.word_tokenize(sentence):\n",
    "        # check to see if the stem of the word is in any of our classes\n",
    "        if stemmer.stem(word.lower()) in class_words[class_name]:\n",
    "            # treat each word with relative weight\n",
    "            score += (1 / corpus_words[stemmer.stem(word.lower())])\n",
    "\n",
    "            if show_details:\n",
    "                print (\"   match: %s (%s)\" % (stemmer.stem(word.lower()), 1 / corpus_words[stemmer.stem(word.lower())]))\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   match: for (1.0)\n",
      "   match: hav (0.3333333333333333)\n",
      "   match: lunch (1.0)\n",
      "Class: sandwich  Score: 2.333333333333333 \n",
      "\n",
      "   match: day (0.25)\n",
      "   match: to (1.0)\n",
      "   match: hav (0.3333333333333333)\n",
      "Class: goodbye  Score: 1.5833333333333333 \n",
      "\n",
      "   match: good (1.0)\n",
      "   match: day (0.25)\n",
      "Class: greeting  Score: 1.25 \n",
      "\n"
     ]
    }
   ],
   "source": [
    " #now we can find the class with the highest score\n",
    "for c in class_words.keys():\n",
    "    print (\"Class: %s  Score: %s \\n\" % (c, calculate_class_score_commonality(sentence, c)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the class with highest score for sentence\n",
    "def classify(sentence):\n",
    "    high_class = None\n",
    "    high_score = 0\n",
    "    # loop through our classes\n",
    "    for c in class_words.keys():\n",
    "        # calculate score of sentence for each class\n",
    "        score = calculate_class_score_commonality(sentence, c, show_details=False)\n",
    "        # keep track of highest score\n",
    "        if score > high_score:\n",
    "            high_class = c\n",
    "            high_score = score\n",
    "\n",
    "    return high_class, high_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('sandwich', 2.5)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify(\"make me some lunch?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('goodbye', 1.75)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify(\"Nice to meet you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, 0)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify(\"softuni\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
